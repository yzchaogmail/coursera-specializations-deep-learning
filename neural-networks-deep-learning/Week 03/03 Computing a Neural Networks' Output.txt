在上一节课里你看到了
单隐层神经网络是什么样子的 在这节课中 我们将详细了解一下 神经网络计算它的输出的细节 你将会发现 我们只是
将逻辑回归进行多次的重复 让我们一起来看看 这是一个双层的神经网络 我们深入了解一下这个神经网络计算了什么 我们在讲逻辑回归的时候曾经说过 在逻辑回归中 一个圆代表了两步计算 首先 你可以按照这种方式计算z 然后 计算sigmoid(z)作为激活函数 神经网络只是把这个过程做了多次 首先我们注意隐藏层里面的一个节点 我们先看隐藏层的第一个节点 我把其他的节点标为灰色 和左边的逻辑回归类似 隐藏层中的节点进行了两步计算 第一步 和左边的这个节点一样 计算了了z=W^T·x+b 我们使用的符号是 关联了第一隐藏层中的所有节点的 这也是为什么这里有一堆方括号
这是隐藏层的第一个节点 所以我们有一个上标1 首先我们做了这个 第二步就像这样计算
a_1^[1]=sigmoid(z_1^[1]) 对于z和a 在符号上的惯例为a^[l]_i 这里的l指的是第l层 这里的i指的则是
第l层中的第i个节点 注意我们刚刚看的是
第一层(隐藏层)中的第一个节点 所以它的上标和下标都是1 这个小圆圈 表示神经网络中的第一个节点 执行了这两步计算 现在我们来看神经网络隐藏层中的第二个节点 和左边的逻辑回归单元类似 这个小圆表示了两步计算 第一步计算z 这里还是第一层 但是已经是第二个节点=W_2^[1]^T·x+b a_2^[1]=σ(z_2^[1]) 你可以暂停视频 再次检查上标和下标 以确保它们遵循了
我们上面约定的书写规则 我们探讨了神经网络中的前两个隐藏单元 第三个和第四个隐藏单元表示了相似的计算 现在 我把这组等式 和这一组等式复制到下一页 这就是我们的神经网络
这是第一个 这是第二个 我们之前计算出的
第一隐藏单元和第二隐藏单元的等式 然后继续写下第三隐藏单元
和第四隐藏单元对应的等式 你就得到了下面的这些等式 明确一下 这是向量W_1^[1] 这是一个向量转置乘以X OK? 所以这里有一个上标T 表示向量转置 你可能猜到了 如果你实际实现一个神经网络 使用for循环来实现它似乎效率很低 所以我们下一步要做的
就是将这四个等式向量化 我们将从如何用向量的方法计算z开始 接下来你可以用这种方法来计算它 将这些W叠放到一个矩阵中 你就得到了w[1],1的转置
这就是一个行向量 或者说 这是一个列向量经过转置所得到的行向量 然后是w[1],2的转置
w[1],3的转置 w[1],4的转置 如此 通过把那四个w向量堆叠在一起
你就得到了一个矩阵 你也可以从另外一个角度来理解
我们现在有四个逻辑回归单元 每一个逻辑回归单元
都有一个相对应的参数向量w 通过堆叠这四个向量 你就得到了这个(4,3)矩阵 所以，如果你用这个矩阵去乘输入变量 x1,x2,x3 通过矩阵乘法运算 你就得到了w1[1]的转置乘以x
w2[1]的转置乘以x w3[1]的转置乘以x w4[1]的转置乘以x
同时别忘了那些b 我们现在把这些b向量加上去
b[1]1,b[1]2 b[1]3,b[1]4 也就是绿色的这些项 那么这里就是b[1]1,b[1]2,b[1]3,b[1]4 你就会看到结果中
每一行都是一一对应于 上面这四行中的一行 换句话说 我们刚刚展示了
我们得到的结果就等于z[1]1 z[1]2,z[1]3,z[1]4，就像这里定义的一样。 或许你已经猜到了
我们将把这一大坨东西叫做 向量Z[1] 向量Z[1]就是把这些单独的Z
堆叠在一起而形成的列向量 当我们进行向量化时
一个经验能帮到你 那就是 在一层中有不同的神经元时
我们就把他们堆叠起来 这就是为什么 当你有Z[1]1到Z[1]4这些 在隐藏层中对应于不同神经元的时候， 我们就把这四个
垂直堆叠起来而形成Z[1]向量 与此同时 这边这个
通过堆叠w[1]1,w[1]2等 而得到的(4,3)矩阵 我们将把这个矩阵称为W[1]
类似的 这边这个向量 我们将把它成为b[1] 这是一个(4,1)向量 到目前为止 我们已经通过
这个向量矩阵计算了向量Z 最后 我们要做的是去计算出这些a 可能你又猜到了
我们将会通过堆叠来定义a 我们把那些激活值a[1]1到a[1]4堆叠起来 就是把这四个值堆叠在一起 得到向量a[1] 这将会是Sigmoid函数作用在
z[1]上后得到的值 这里这个Sigmoid函数 将会接受z[1]中的每个元素
然后用Sigmoid函数来进行运算 来复习一下 我们通过运算得出 z[1]=W[1]x+b[1] 同时a[1]=Sigmoid*z[1] 我们把这个拷贝到下一页 我们会看到 对于第一层神经网络来说
如果输入是一组x (x向量) 我们就有z[1]=W[1]x+b[1]，同时 a[1]=σ(z[1]) (4,1) = (4,3) x (3,1) + (4,1) 这个的维度是(4,1) 和最后的维度是相同的 还记得吧 我们说过x=a[0] y^=a[2] 所以如果你想的话 你可以用a[0]来替代x 因为a[0]就是x向量的别名。 现在 通过类似的推导 你可以得出
下一层的表达式 这个表达式和第一层的表达式十分类似 输出层(就是z[2])的参数W[2]和b[2] 这里的W[2]是一个(1,4)矩阵 b[2]就是一个实数 你也可以说它是个(1,1)矩阵 z[2]最后也是一个实数 也可以把它写成(1,1)矩阵 这个W[2]是个(1,4)矩阵 用它去乘
a[1]这个(4,1)矩阵 然后加b[2]这个(1,1)矩阵 所以 最后结果就是个实数
然后如果你把最后的输出项 去和逻辑回归做一个类比
(逻辑回归有参数W和b) 你会发现在逻辑回归运算里面的W其实就是起着
神经网络运算里面W[2]转置的作用 或者说 (神经网络的)W[2]就是(逻辑回归的)W的转置，
同时(逻辑回归的)b就等于(神经网络的)b[2] 现在 如果我们把左边这个部分都挡住不看 那么 这最后的输出单元就与逻辑回归非常相似了 有些不同的就是 我们现在不写(逻辑回归参数)W和b 我们写(神经网络的)W[2]和b[2]
它们的维度分别是(1,4)和(1,1) 来复习一下 对于逻辑回归来说 去实现一个输出 或者说去预测一个结果
你会计算z=w^T*x+b y^等于a 等于 sigmoid of (z). 
(注：y^就等于a[2]) 当你有一个神经网络中
有一个隐藏层 你需要做的 就是通过这四个运算去计算最终的输出 你可以把这些运算想象成
第一步--通过向量化运算 得出隐藏层a[1]里这四个逻辑回归的输出
(注：这四个输出是下一层a[2]的输入) 第二步--用a[1]的四个输出作为a[2]层的输入
就像第三第四个运算做的一样 希望你能理解我的表述
其实 你要掌握的就是 你只需要那四行运算代码
去计算神经网络的输出 现在你知道了 给你一个输入参量x 你可以用四行代码就计算出
这个神经网络的输出。 然后与逻辑回归类似 我们也想实现出一个 能够同时在多个样本上
进行运算的向量化算法 我们会看到 通过把训练样本堆叠在
矩阵的不同列中 只要通过很小的改变 我们就能将逻辑回归中的
向量化实现照搬过来 以使得神经网络不仅能
计算单个样本上的输出值 而且能计算整个训练样本集上的输出值 我们将在下一节课中讨论这些细节
GTC字幕组翻译